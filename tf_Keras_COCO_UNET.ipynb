{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_Keras_COCO_UNET.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-QreKZI06NT",
        "colab_type": "text"
      },
      "source": [
        "# COCO DATASET GATHERING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3DkfDsiwrpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!unzip -q train2014.zip\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!unzip -q val2014.zip\n",
        "!unzip -q annotations_trainval2014.zip\n",
        "\n",
        "\n",
        "! pip install 2to3\n",
        "!git clone https://github.com/cocodataset/cocoapi.git\n",
        "%cd cocoapi\n",
        "!2to3 . -w\n",
        "%cd PythonAPI\n",
        "!python3 setup.py install\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT6zZ-D51LWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnF0aYFB1OXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Data From COCO\n",
        "\n",
        "from pycocotools import coco, cocoeval, _mask\n",
        "from pycocotools import mask as maskUtils \n",
        "import array\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "import os\n",
        "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUWcLqnJ1SUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORY_NAMES=['person']\n",
        "\n",
        "ANNOTATION_FILE_VAL = '/content/annotations/instances_val2014.json'\n",
        "ANNOTATION_FILE_TRAIN = '/content/annotations/instances_train2014.json'\n",
        "\n",
        "\n",
        "coco_train = coco.COCO(ANNOTATION_FILE_TRAIN)\n",
        "catIds_train = coco_train.getCatIds(catNms=CATEGORY_NAMES);\n",
        "imgIds_train = coco_train.getImgIds(catIds=catIds_train);\n",
        "imgDict_train = coco_train.loadImgs(imgIds_train)\n",
        "len(imgIds_train) , len(catIds_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "coco_val = coco.COCO(ANNOTATION_FILE_VAL)\n",
        "catIds_val = coco_val.getCatIds(catNms=CATEGORY_NAMES);\n",
        "imgIds_val = coco_val.getImgIds(catIds=catIds_val);\n",
        "imgDict_val = coco_val.loadImgs(imgIds_val)\n",
        "len(imgIds_val) , len(catIds_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9akfTem1Ue-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import shuffle\n",
        "\n",
        "\n",
        "shuffle(imgIds_train)\n",
        "shuffle(imgIds_val)\n",
        "\n",
        "imgIds_train = imgIds_train[0:6000]\n",
        "imgIds_val = imgIds_val[0:600]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIktAjPp1Xs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images_person = [\"COCO_train2014_{0:012d}.jpg\".format(ids) for ids in imgIds_train]\n",
        "val_images_person = [\"COCO_val2014_{0:012d}.jpg\".format(ids) for ids in imgIds_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD2mRyK71bSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_images_person) , len(val_images_person))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDxYUzZT1crY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images_person = [\"COCO_train2014_{0:012d}.jpg\".format(ids) for ids in imgIds_train]\n",
        "print(train_images_person)\n",
        "del_img_train = set(os.listdir(\"/content/train2014\")) - set(train_images_person)\n",
        "for file_name in del_img_train:\n",
        "  file_name = \"/content/train2014/\" + file_name\n",
        "  if os.path.exists(file_name):\n",
        "    os.remove(file_name)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gWAno9N1dUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(os.listdir(\"/content/train2014\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAeyJbE71fUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_images_person = [\"COCO_val2014_{0:012d}.jpg\".format(ids) for ids in imgIds_val]\n",
        "print(val_images_person)\n",
        "del_img_val = set(os.listdir(\"/content/val2014\")) - set(val_images_person)\n",
        "for file_name in del_img_val:\n",
        "  file_name = \"/content/val2014/\" + file_name\n",
        "  if os.path.exists(file_name):\n",
        "    os.remove(file_name)\n",
        "\n",
        "len(os.listdir(\"/content/val2014\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyiCRQGd1hVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHhhJuEw1mvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir mask_train_2014"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W4zM_SZ1new",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0 \n",
        "\n",
        "for ID in imgIds_train:\n",
        "\n",
        "  file_path = \"/content/mask_train_2014/COCO_train2014_{0:012d}.jpg\".format(ID)\n",
        "  \n",
        "  sampleImgIds = coco_train.getImgIds(imgIds = [ID])\n",
        "  sampleImgDict = coco_train.loadImgs(sampleImgIds[np.random.randint(0,len(sampleImgIds))])[0]\n",
        "\n",
        "  annIds = coco_train.getAnnIds(imgIds=sampleImgDict['id'], catIds=catIds_train, iscrowd=0)\n",
        "  anns = coco_train.loadAnns(annIds)\n",
        "\n",
        "\n",
        "  mask = coco_train.annToMask(anns[0])\n",
        "  for i in range(len(anns)):\n",
        "      mask = mask | coco_train.annToMask(anns[i])\n",
        "  \n",
        "  mask = Image.fromarray(mask * 255 , mode = \"L\")\n",
        "  mask.save(file_path)\n",
        "  count = count + 1\n",
        "  print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBAlOCAg1s02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir mask_val_2014"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdcMDJGY1ui7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0 \n",
        "for ID in imgIds_val:\n",
        "\n",
        "  file_path = \"/content/mask_val_2014/COCO_val2014_{0:012d}.jpg\".format(ID)\n",
        "  \n",
        "  sampleImgIds = coco_val.getImgIds(imgIds = [ID])\n",
        "  sampleImgDict = coco_val.loadImgs(sampleImgIds[np.random.randint(0,len(sampleImgIds))])[0]\n",
        "\n",
        "  annIds = coco_val.getAnnIds(imgIds=sampleImgDict['id'], catIds=catIds_val, iscrowd=0)\n",
        "  anns = coco_val.loadAnns(annIds)\n",
        "\n",
        "\n",
        "  mask = coco_val.annToMask(anns[0])\n",
        "  for i in range(len(anns)):\n",
        "      mask = mask | coco_val.annToMask(anns[i])\n",
        "  \n",
        "  mask = Image.fromarray(mask * 255 , mode = \"L\")\n",
        "  mask.save(file_path)\n",
        "  \n",
        "  count = count + 1\n",
        "  print(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-OMmybC1v1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf annotations/\n",
        "!rm -rf train2014.zip\n",
        "!rm -rf val2014.zip\n",
        "!rm -rf annotations_trainval2014.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_rmAwLIezXG",
        "colab_type": "text"
      },
      "source": [
        "# Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJbmPHNceazg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "\n",
        "\n",
        "seed = 2019\n",
        "\n",
        "random.seed = seed\n",
        "np.random.seed = seed "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrV1_sQOevZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGen(tf.keras.utils.Sequence):\n",
        "  \n",
        "  def __init__(self , path_input , path_mask , batch_size = 8 , image_size = 128):\n",
        "    \n",
        "    self.ids = os.listdir(path_input)\n",
        "    self.path_input = path_input\n",
        "    self.path_mask = path_mask\n",
        "    self.batch_size = batch_size\n",
        "    self.image_size = image_size\n",
        "    self.on_epoch_end()\n",
        "  \n",
        "  def __load__(self , id_name):\n",
        "    \n",
        "    image_path = os.path.join(self.path_input , id_name)\n",
        "    mask_path = os.path.join(self.path_mask , id_name) \n",
        "    \n",
        "    image = cv2.imread(image_path , 1) # 1 specifies RGB format\n",
        "    image = cv2.resize(image , (self.image_size , self.image_size)) # resizing before inserting to the network\n",
        "    \n",
        "    mask = cv2.imread(mask_path , -1)\n",
        "    mask = cv2.resize(mask , (self.image_size , self.image_size))\n",
        "    mask = mask.reshape((self.image_size , self.image_size , 1))\n",
        "      \n",
        "    #normalize image\n",
        "    image = image / 255.0\n",
        "    mask = mask / 255.0\n",
        "    \n",
        "    return image , mask\n",
        "  \n",
        "  def __getitem__(self , index):\n",
        "    \n",
        "    if (index + 1)*self.batch_size > len(self.ids):\n",
        "      self.batch_size = len(self.ids) - index * self.batch_size\n",
        "        \n",
        "    file_batch = self.ids[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "    \n",
        "    images = []\n",
        "    masks = []\n",
        "    \n",
        "    for id_name in file_batch : \n",
        "      \n",
        "      _img , _mask = self.__load__(id_name)\n",
        "      images.append(_img)\n",
        "      masks.append(_mask)\n",
        "    \n",
        "    \n",
        "    images = np.array(images)\n",
        "    masks = np.array(masks)\n",
        "    \n",
        "    \n",
        "    return images , masks\n",
        "  \n",
        "  \n",
        "  def on_epoch_end(self):\n",
        "    pass\n",
        "  \n",
        "  \n",
        "  def __len__(self):\n",
        "    \n",
        "    return int(np.ceil(len(self.ids) / float(self.batch_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L3_Slh1e2vZ",
        "colab_type": "text"
      },
      "source": [
        "#UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65_xh2VYe2M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def down_block(\n",
        "    input_tensor,\n",
        "    no_filters,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    padding=\"same\",\n",
        "    kernel_initializer=\"he_normal\",\n",
        "    max_pool_window=(2, 2),\n",
        "    max_pool_stride=(2, 2)\n",
        "):\n",
        "    conv = Conv2D(\n",
        "        filters=no_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        activation=None,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer\n",
        "    )(input_tensor)\n",
        "\n",
        "    conv = BatchNormalization(scale=True)(conv)\n",
        "\n",
        "    conv = Activation(\"relu\")(conv)\n",
        "\n",
        "    conv = Conv2D(\n",
        "        filters=no_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        activation=None,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer\n",
        "    )(conv)\n",
        "\n",
        "    conv = BatchNormalization(scale=True)(conv)\n",
        "\n",
        "    # conv for skip connection\n",
        "    conv = Activation(\"relu\")(conv)\n",
        "\n",
        "    pool = MaxPooling2D(pool_size=max_pool_window, strides=max_pool_stride)(conv)\n",
        "\n",
        "    return conv, pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FaVzvm3fESd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bottle_neck(\n",
        "    input_tensor,\n",
        "    no_filters,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    padding=\"same\",\n",
        "    kernel_initializer=\"he_normal\"\n",
        "):\n",
        "    conv = Conv2D(\n",
        "        filters=no_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        activation=None,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer\n",
        "    )(input_tensor)\n",
        "\n",
        "    conv = BatchNormalization(scale=True)(conv)\n",
        "\n",
        "    conv = Activation(\"relu\")(conv)\n",
        "\n",
        "    conv = Conv2D(\n",
        "        filters=no_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        activation=None,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer\n",
        "    )(conv)\n",
        "\n",
        "    conv = BatchNormalization(scale=True)(conv)\n",
        "\n",
        "    conv = Activation(\"relu\")(conv)\n",
        "\n",
        "    return conv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSzLN6m6fGMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def up_block(    \n",
        "    input_tensor,\n",
        "    no_filters,\n",
        "    skip_connection, \n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    upsampling_factor = (2,2),\n",
        "    max_pool_window = (2,2),\n",
        "    padding=\"same\",\n",
        "    kernel_initializer=\"he_normal\"):\n",
        "    \n",
        "    \n",
        "    conv = Conv2D(\n",
        "        filters = no_filters,\n",
        "        kernel_size= max_pool_window,\n",
        "        strides = strides,\n",
        "        activation = None,\n",
        "        padding = padding,\n",
        "        kernel_initializer=kernel_initializer\n",
        "    )(UpSampling2D(size = upsampling_factor)(input_tensor))\n",
        "    \n",
        "    conv = BatchNormalization(scale=True)(conv)\n",
        "\n",
        "    conv = Activation(\"relu\")(conv) \n",
        "    \n",
        "    \n",
        "    conv = concatenate( [skip_connection , conv]  , axis = -1)\n",
        "    \n",
        "    \n",
        "    conv = Conv2D(\n",
        "        filters=no_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        activation=None,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer\n",
        "    )(conv)\n",
        "\n",
        "    conv = BatchNormalization(scale=True)(conv)\n",
        "\n",
        "    conv = Activation(\"relu\")(conv)\n",
        "\n",
        "    conv = Conv2D(\n",
        "        filters=no_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        activation=None,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer\n",
        "    )(conv)\n",
        "\n",
        "    conv = BatchNormalization(scale=True)(conv)\n",
        "\n",
        "    conv = Activation(\"relu\")(conv)\n",
        "    \n",
        "    return conv\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LUtzcZVfG3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_block(input_tensor,\n",
        "    padding=\"same\",\n",
        "    kernel_initializer=\"he_normal\"\n",
        "):\n",
        "    \n",
        "    conv = Conv2D(\n",
        "        filters=2,\n",
        "        kernel_size=(3,3),\n",
        "        strides=(1,1),\n",
        "        activation=\"relu\",\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer\n",
        "    )(input_tensor)\n",
        "    \n",
        "    \n",
        "    conv = Conv2D(\n",
        "        filters=1,\n",
        "        kernel_size=(1,1),\n",
        "        strides=(1,1),\n",
        "        activation=\"sigmoid\",\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer\n",
        "    )(conv)\n",
        "    \n",
        "    \n",
        "    return conv\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOEfWxYnfJho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UNet(input_shape = (128,128,3)):\n",
        "    \n",
        "    filter_size = [64,128,256,512,1024]\n",
        "    \n",
        "    inputs = Input(shape = input_shape)\n",
        "    \n",
        "    d1 , p1 = down_block(input_tensor= inputs,\n",
        "                         no_filters=filter_size[0],\n",
        "                         kernel_size = (3,3),\n",
        "                         strides=(1,1),\n",
        "                         padding=\"same\",\n",
        "                         kernel_initializer=\"he_normal\",\n",
        "                         max_pool_window=(2,2),\n",
        "                         max_pool_stride=(2,2))\n",
        "    \n",
        "    \n",
        "    d2 , p2 = down_block(input_tensor= p1,\n",
        "                         no_filters=filter_size[1],\n",
        "                         kernel_size = (3,3),\n",
        "                         strides=(1,1),\n",
        "                         padding=\"same\",\n",
        "                         kernel_initializer=\"he_normal\",\n",
        "                         max_pool_window=(2,2),\n",
        "                         max_pool_stride=(2,2))\n",
        "    \n",
        "    \n",
        "    \n",
        "    d3 , p3 = down_block(input_tensor= p2,\n",
        "                         no_filters=filter_size[2],\n",
        "                         kernel_size = (3,3),\n",
        "                         strides=(1,1),\n",
        "                         padding=\"same\",\n",
        "                         kernel_initializer=\"he_normal\",\n",
        "                         max_pool_window=(2,2),\n",
        "                         max_pool_stride=(2,2))\n",
        "    \n",
        "    \n",
        "    \n",
        "    d4 , p4 = down_block(input_tensor= p3,\n",
        "                         no_filters=filter_size[3],\n",
        "                         kernel_size = (3,3),\n",
        "                         strides=(1,1),\n",
        "                         padding=\"same\",\n",
        "                         kernel_initializer=\"he_normal\",\n",
        "                         max_pool_window=(2,2),\n",
        "                         max_pool_stride=(2,2))\n",
        "    \n",
        "    \n",
        "    b = bottle_neck(input_tensor= p4,\n",
        "                         no_filters=filter_size[4],\n",
        "                         kernel_size = (3,3),\n",
        "                         strides=(1,1),\n",
        "                         padding=\"same\",\n",
        "                         kernel_initializer=\"he_normal\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    u4 = up_block(input_tensor = b,\n",
        "                  no_filters = filter_size[3],\n",
        "                  skip_connection = d4,\n",
        "                  kernel_size=(3, 3),\n",
        "                  strides=(1, 1),\n",
        "                  upsampling_factor = (2,2),\n",
        "                  max_pool_window = (2,2),\n",
        "                  padding=\"same\",\n",
        "                  kernel_initializer=\"he_normal\")\n",
        "    \n",
        "    u3 = up_block(input_tensor = u4,\n",
        "                  no_filters = filter_size[2],\n",
        "                  skip_connection = d3,\n",
        "                  kernel_size=(3, 3),\n",
        "                  strides=(1, 1),\n",
        "                  upsampling_factor = (2,2),\n",
        "                  max_pool_window = (2,2),\n",
        "                  padding=\"same\",\n",
        "                  kernel_initializer=\"he_normal\")\n",
        "    \n",
        "    \n",
        "    u2 = up_block(input_tensor = u3,\n",
        "                  no_filters = filter_size[1],\n",
        "                  skip_connection = d2,\n",
        "                  kernel_size=(3, 3),\n",
        "                  strides=(1, 1),\n",
        "                  upsampling_factor = (2,2),\n",
        "                  max_pool_window = (2,2),\n",
        "                  padding=\"same\",\n",
        "                  kernel_initializer=\"he_normal\")\n",
        "    \n",
        "    \n",
        "    u1 = up_block(input_tensor = u2,\n",
        "                  no_filters = filter_size[0],\n",
        "                  skip_connection = d1,\n",
        "                  kernel_size=(3, 3),\n",
        "                  strides=(1, 1),\n",
        "                  upsampling_factor = (2,2),\n",
        "                  max_pool_window = (2,2),\n",
        "                  padding=\"same\",\n",
        "                  kernel_initializer=\"he_normal\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    output = output_block(input_tensor=u1 , \n",
        "                         padding = \"same\",\n",
        "                         kernel_initializer= \"he_normal\")\n",
        "    \n",
        "    model = Model(inputs = inputs , outputs = output)\n",
        "    \n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVbsBQnJfOlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = UNet(input_shape = (128,128,3))\n",
        "model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpPfRQyrfVtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = 128 \n",
        "epochs = 10\n",
        "batch_size = 8\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7uP2BG9gX9Y",
        "colab_type": "code",
        "outputId": "36e73a47-3b49-411d-f198-acab92641f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "train_gen = DataGen(path_input = \"/content/train2014\" , path_mask = \"/content/mask_train_2014/\" , batch_size = batch_size , image_size = image_size)\n",
        "val_gen = DataGen(path_input =  \"/content/val2014\", path_mask =  \"/content/mask_val_2014\", batch_size = batch_size , image_size = image_size)\n",
        "\n",
        "\n",
        "train_steps =  len(os.listdir( \"/content/train2014\"))/batch_size\n",
        "\n",
        "\n",
        "model.fit_generator(train_gen , validation_data = val_gen , steps_per_epoch = train_steps , epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "750/750 [==============================] - 344s 458ms/step - loss: 0.6813 - acc: 0.8204 - val_loss: 0.6685 - val_acc: 0.8300\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 335s 447ms/step - loss: 0.6576 - acc: 0.8230 - val_loss: 0.6456 - val_acc: 0.8302\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 336s 447ms/step - loss: 0.6352 - acc: 0.8254 - val_loss: 0.6708 - val_acc: 0.6962\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 336s 447ms/step - loss: 0.6093 - acc: 0.8384 - val_loss: 0.6203 - val_acc: 0.8332\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 335s 447ms/step - loss: 0.5830 - acc: 0.8499 - val_loss: 0.5680 - val_acc: 0.8603\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 335s 447ms/step - loss: 0.5579 - acc: 0.8583 - val_loss: 0.5567 - val_acc: 0.8549\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 336s 447ms/step - loss: 0.5340 - acc: 0.8650 - val_loss: 0.5211 - val_acc: 0.8713\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 335s 447ms/step - loss: 0.5114 - acc: 0.8700 - val_loss: 0.5028 - val_acc: 0.8731\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 335s 446ms/step - loss: 0.4891 - acc: 0.8756 - val_loss: 0.4833 - val_acc: 0.8750\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 335s 447ms/step - loss: 0.4672 - acc: 0.8811 - val_loss: 0.4644 - val_acc: 0.8791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fda9d912358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykqwnhtNhtFe",
        "colab_type": "code",
        "outputId": "11627c61-188e-4bf4-b3b9-fdaec1921fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "x, y = val_gen.__getitem__(4)\n",
        "result = model.predict(x)\n",
        "\n",
        "result = result > 0.5\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.imshow(np.reshape(y[0]*255, (image_size, image_size)), cmap=\"gray\")\n",
        "\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.imshow(np.reshape(result[0]*255, (image_size, image_size)), cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fda8e22b898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAACuCAYAAAA4eMYdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDL0gCELK+f8VXsEJ7jV8/wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQqf0lvjk-jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}